{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6756f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "repo_dir = os.environ.get(\"REPO_DIR\")\n",
    "code_dir = os.path.join(repo_dir, \"code/\")\n",
    "data_dir = os.path.join(repo_dir, \"data/\")\n",
    "\n",
    "os.chdir(code_dir)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import pickle\n",
    "import sklearn \n",
    "import sys\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import spearmanr, mode\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import zarr\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "\n",
    "from mosaiks.utils.imports import *\n",
    "\n",
    "from mosaiks.utils.io import weighted_groupby\n",
    "from affine import Affine\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "# Key prediction functions are here\n",
    "from prediction_utils import (X_matrix_to_demeaned_X,df_to_demeaned_y_vars, flatten_raster,rasterize_df,\n",
    "make_train_pred_scatterplot as make_scatterplot, cv_solve, solver_kwargs, get_truth_preds_from_kfold_results,\n",
    "                             predict_y_from_kfold_dict, generalized_demean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb5a86",
   "metadata": {},
   "source": [
    "# Predicting grid level HDI\n",
    "\n",
    "In this notebook, we generate HDI at the grid level. This notebook is here for reference, but executing it would require the MOSAIKS features at the native tile level (.01 x .01 degrees) for the globe (3TB). We cannot make data of this size easily downloadable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91749b",
   "metadata": {},
   "source": [
    "### First, we need nightlight values at the .01 x .01 degree (tile) level\n",
    "\n",
    "This file is created in `code/analysis/NL_feature_creation_and_other_NL_processing/nighlight_features_for_hdi_labels_DMSP.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24656d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (data_dir + \"int/DMSP_NL/\"\n",
    "           \"F182013.v4c_web.stable_lights.avg_vis_converted_to_np-float.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e170414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rasterio.open(path)\n",
    "arr = src.read(1)\n",
    "x,y,vals = flatten_raster(arr, src.transform)\n",
    "nl = pd.DataFrame({\"lon\":x,\"lat\":y,\"nl\":vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a01287",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert .00833 res to .001 res\n",
    "nl[\"lon\"] = np.round(np.round(nl[\"lon\"] - .005,2) + .005,3)\n",
    "nl[\"lat\"] = np.round(np.round(nl[\"lat\"] + .005,2) - .005,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfa41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = nl.groupby([\"lon\",\"lat\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nl[\"lon\"].min(),nl[\"lon\"].max(),nl[\"lat\"].min(),nl[\"lat\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.hstack([0,np.linspace(0.0,63,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce741c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned(a):\n",
    "    d =  np.histogram(a, bins=bins, density = False, weights=None)\n",
    "    perc_in_each_bin = d[0]\n",
    "    return perc_in_each_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0b2bb",
   "metadata": {},
   "source": [
    "### Now we also need population weights\n",
    "This population weights file is created in the `population_weights_for_features_and_grid_preds.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1924dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_pickle(data_dir + \"int/GPW_pop/\" +\n",
    "                               \"/population_density_global_dense_grid.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc55ec",
   "metadata": {},
   "source": [
    "### We need the country dense grid tiles for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_dir  + \"features/prepared_labels/GDL_HDI_polygon_coords_for_featurization.p\")\n",
    "hdi = pd.read_pickle(data_dir +\"int/GDL_HDI/HDI_indicators_and_indices_clean.p\")[[\"Sub-national HDI\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276751b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(hdi, \"left\", left_on=\"GDLCODE\", right_index=True)\n",
    "df.set_index([\"lon\",\"lat\"], inplace=True)\n",
    "df = df.merge(nl, \"left\", left_index=True, right_on = [\"lon\", \"lat\"])\n",
    "df = df.merge(pop, \"left\", on = [\"lon\", \"lat\"])\n",
    "df.set_index([\"lon\",\"lat\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d95e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d600b",
   "metadata": {},
   "source": [
    "### We also need the country level features that we used to demean in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_X = pd.read_pickle(\n",
    "(data_dir + \"features/mosaiks_features/\"\n",
    "\"GDL_ADM1_polygon_X_creation_pop_weight=True.p\")).drop(columns=\"GDLCODE\")\n",
    "\n",
    "filepath = (data_dir + \"features/nl_features/GDL_HDI_polygons/\"\n",
    "\"dmsp_nightlight_features_for_hdi_polygons_20_bins_GPW_pop_weighted.p\")\n",
    "adm1_nl = pd.read_pickle(filepath).loc[adm1_X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demean_vectors = X_matrix_to_demeaned_X(adm1_X, return_mean_frame=True )\n",
    "X_demean_vectors_nl = X_matrix_to_demeaned_X(adm1_nl, return_mean_frame=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demean_vectors_nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1bf25",
   "metadata": {},
   "source": [
    "### And finally, we need the primary model used for preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (data_dir + \"/model_data/\" +\n",
    "           \"within_country_rcf_and_nl_demeaned_solve_all_outcomes_country_fold\"\n",
    "           \"_DENSE_pop_weight=True_dmsp_hist_bins_GPW_pop_weighted.pkl\")\n",
    "\n",
    "nl_and_rcf_demeaned_kfold_dict = pickle.load(open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107546d6",
   "metadata": {},
   "source": [
    "### Now we cycle through all the dense grid chunks and produce predictions for those associated with a country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c5f35",
   "metadata": {},
   "source": [
    "These source files in `z_directory` and the intermediate output files in `int_dir` are not publicly available. They source MOSAIKS features at the 0.01 by 0.01 level total about 3 TB which makes them impractical to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548978c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data directory is outside the GITHUB repo and is not publicy accesible\n",
    "z_directory = \"/shares/maps100/data/features/global_dense_grid/complete/concat/replace_2022/\"\n",
    "int_dir = data_dir + \"/preds/int_grid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdbb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_slices = []\n",
    "\n",
    "for file in os.listdir(z_directory):\n",
    "    if not file.endswith(\".zarr\"):\n",
    "        continue\n",
    "    print(file)\n",
    "    \n",
    "    int_path = int_dir + file.split(\".\")[0]+\"_int_hdi_grid_preds.p\"\n",
    "    \n",
    "    if os.path.exists(int_path):\n",
    "        grid_slices.append(pd.read_pickle(int_path))\n",
    "        continue\n",
    "    \n",
    "    z = zarr.load(z_directory + file)\n",
    "    z = pd.DataFrame(z)\n",
    "    z = z.rename(columns = {0:\"lon\",1:\"lat\"})\n",
    "    z[\"lon\"], z[\"lat\"] = z[\"lon\"].round(3), z[\"lat\"].round(3)\n",
    "    \n",
    "    z.set_index([\"lon\",\"lat\"], inplace=True)\n",
    "    rcf_cols = \"X_\" + np.arange(4000).astype(str).astype(object)\n",
    "    z.columns = rcf_cols\n",
    "    \n",
    "    subset = df.merge(z, how=\"inner\",left_index=True, right_index=True )\n",
    "    countries_in_slice = subset[\"iso_code\"].unique()\n",
    "    \n",
    "    file_data = []\n",
    "    for country in countries_in_slice:\n",
    "        print(country)\n",
    "    \n",
    "        country_subset = subset[subset[\"iso_code\"] == country]\n",
    "    \n",
    "        ## Demeaned X1 for country\n",
    "        rcf_X_country = country_subset.loc[:,\"X_0\":] - X_demean_vectors.loc[country]\n",
    "        \n",
    "        X_country_nl = pd.DataFrame(np.vstack(country_subset[\"nl\"].apply(binned)) - X_demean_vectors_nl.loc[country].to_numpy(),\n",
    "                               index= country_subset.index)\n",
    "    \n",
    "    \n",
    "        country_preds_clipped = predict_y_from_kfold_dict(rcf_X_country,\n",
    "                           nl_and_rcf_demeaned_kfold_dict,\n",
    "                           \"Sub-national HDI\",\n",
    "                           X_country_nl,\n",
    "                          clip_preds=True)\n",
    "        \n",
    "        country_preds_not_clipped = predict_y_from_kfold_dict(rcf_X_country,\n",
    "                           nl_and_rcf_demeaned_kfold_dict,\n",
    "                           \"Sub-national HDI\",\n",
    "                           X_country_nl,\n",
    "                          clip_preds=False)\n",
    "        \n",
    "        country_subset.drop(columns = rcf_cols, inplace=True)\n",
    "        \n",
    "        country_subset[\"raw_pred_hdi\"] = country_preds_clipped\n",
    "        country_subset[\"raw_pred_hdi_not_clipped\"] = country_preds_not_clipped\n",
    "        file_data.append(country_subset)\n",
    "        gc.collect()\n",
    "    \n",
    "    int_output = pd.concat(file_data)\n",
    "    \n",
    "    int_output.to_pickle(int_path)\n",
    "    \n",
    "    grid_slices.append(int_output)    \n",
    "    \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(grid_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_pickle(data_dir + \"preds/raw_hdi_preds_at_grid.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a1a31",
   "metadata": {},
   "source": [
    "# Now we are going to re-center and up-sample the raw grid estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8dd1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(data_dir + \"preds/raw_hdi_preds_at_grid.p\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca35534",
   "metadata": {},
   "source": [
    "### Use finer resolution human settlement population. \n",
    "We do not want to release predictions for locations where people do not live. For these we will use the Global Human Settlement population data layer available for download at https://ghsl.jrc.ec.europa.eu/download.php?ds=pop\n",
    "\n",
    "Specifically this is the following data product:\n",
    "\n",
    "**GHS population grid (R2022)**\n",
    "\n",
    "Product: GHS-POP, epoch: 2020, resolution: 1 km, coordinate system: Mollweide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb1146",
   "metadata": {},
   "source": [
    "#### First we need to re-project from Mollweide to WGS84 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a40ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = data_dir + \"GHS_pop/\"\n",
    "file = \"GHS_POP_E2020_GLOBE_R2022A_54009_1000_V1_0.tif\"\n",
    "dst_crs = 'EPSG:4326'\n",
    "\n",
    "with rasterio.open(directory+file) as src:\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "    \n",
    "    with rasterio.open(directory + \"GHS_POP_E2020_GLOBE_R2022A_54009_1000_V1_0_re-project.tif\", 'w', **kwargs) as dst:\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=rasterio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6280d",
   "metadata": {},
   "source": [
    "#### Now we can continue using the GHS Pop product to create a population mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe64a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsdl = rasterio.open(data_dir + \"int/GHS_pop/\" +\n",
    "                      \"GHS_POP_E2020_GLOBE_R2022A_54009_1000_V1_0_re-project.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0845cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsdl_arr = hsdl.read(1)\n",
    "\n",
    "hsdl_df = flatten_raster(hsdl_arr, hsdl.transform)\n",
    "hsdl_df = pd.DataFrame({\"lon\":hsdl_df[0],\"lat\":hsdl_df[1],\"hsdl_pop\":hsdl_df[2]})\n",
    "\n",
    "hsdl_df[\"pop_binary\"] = (hsdl_df[\"hsdl_pop\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a83ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsdl_df[\"lon01\"] = np.round(np.round(hsdl_df[\"lon\"] + .005,2) - .005,3)\n",
    "hsdl_df[\"lat01\"] = np.round(np.round(hsdl_df[\"lat\"] + .005,2) - .005,3)\n",
    "hsdl_df_grp = hsdl_df.groupby([\"lon01\",\"lat01\"])[\"pop_binary\"].agg(np.nansum).reset_index()\n",
    "\n",
    "hsdl_df_grp[\"pop_binary\"] = (hsdl_df_grp[\"pop_binary\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crop the flat raster bounds to be the same as the prediction bounds. Will result in identically shaped rasters\n",
    "hsdl_df_grp = hsdl_df_grp[ (hsdl_df_grp[\"lat01\"] >= data[\"lat\"].min() ) & \\\n",
    "                          (hsdl_df_grp[\"lat01\"] <= data[\"lat\"].max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebfac952",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save intermediate output\n",
    "#hsdl_df_grp.to_pickle(data_dir + \"int/GHS_pop/hsdl_pop_at_01_grid.p\")\n",
    "hsdl_df_grp = pd.read_pickle(data_dir + \"int/GHS_pop/hsdl_pop_at_01_grid.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a25e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(hsdl_df_grp, \n",
    "                   \"left\", left_on = [\"lon\",\"lat\"], right_on=[\"lon01\",\"lat01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3f53f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save a second intermediate output\n",
    "#data.to_pickle(data_dir + \"preds/raw_hdi_preds_at_grid_with_hsdl.p\")\n",
    "data = pd.read_pickle(data_dir + \"preds/raw_hdi_preds_at_grid_with_hsdl.p\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop preds that do not have 0 population in the HSDL dataset\n",
    "data = data[data[\"pop_binary\"] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign smallest positive pop density weight to remaining locations where pop density weights were NaN\n",
    "data.loc[data[\"population\"].isnull(), \"population\"] = data[\"population\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns =[\"lon01\",\"lat01\"], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60977a",
   "metadata": {},
   "source": [
    "## Re-center preds on the known ADM1 Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = io.weighted_groupby(data, \"GDLCODE\", weights_col_name=\"population\", cols_to_agg=[\"raw_pred_hdi\"] )\n",
    "grouped.rename(columns = {\"raw_pred_hdi\":\"weighted_avg_raw\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79355b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(grouped, left_on=\"GDLCODE\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd431d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"adj_factor\"] = data[\"Sub-national HDI\"] - data[\"weighted_avg_raw\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"centered_pred\"] = data[\"raw_pred_hdi\"] + data[\"adj_factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to check that groupby worked\n",
    "#weighted_groupby(data.dropna(), \"GDLCODE\", weights_col_name=\"population\", cols_to_agg=[\"centered_pred\",\"Sub-national HDI\"] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a64b7",
   "metadata": {},
   "source": [
    "## Rasterize and upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c2f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"lat10\"] = np.round(np.round(data[\"lat\"] + .05,1) - .05,2)\n",
    "data[\"lon10\"] = np.round(np.round(data[\"lon\"] + .05,1) - .05,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_raster = data.groupby([\"lon10\",\"lat10\"])[[\"centered_pred\",\"population\",\"Sub-national HDI\",\"GDLCODE\"]].agg(\n",
    "    {\n",
    "    \"population\": np.nansum, # Sum the weights\n",
    "    \"Sub-national HDI\": lambda x: mode(x, nan_policy=\"omit\")[0], # For this col, keep the modal HDI\n",
    "     \"GDLCODE\": lambda x: mode(x,nan_policy=\"omit\")[0], # For this col, keep the modal parent ADM1 code\n",
    "    }) #ignore NaNs for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e431a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now for HDI we want to take the weighted average of the cells, \n",
    "# using the same GPW pop density weights that we have been using throughout\n",
    "pre_raster = pd.concat( [pre_raster,weighted_groupby(data, \n",
    "                                                   [\"lon10\",\"lat10\"], \n",
    "                                                   \"population\", \n",
    "                                                   cols_to_agg = [\"centered_pred\"]\n",
    "                                                  )\n",
    "                       ],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( round((pre_raster[\"centered_pred\"] > 1).sum() / len(pre_raster) * 100,3),\"% of pixels have values higher than 1. We will clip these\")\n",
    "\n",
    "## Apply clipping because HDI should not exceed 1. \n",
    "#This is an imperfect solution, but we will accept since the number of pixels is so small\n",
    "pre_raster[\"clipped\"] = np.clip(pre_raster[\"centered_pred\"],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdaf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_raster = pre_raster.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_raster.to_pickle(data_dir + \"preds/\"\n",
    "           \"hdi_grid_predictions_flat_file.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that weighted aggregation matches\n",
    "# NOTE that this will not be perfect becausecountry borders that are smoothed over\n",
    "#weighted_groupby(pre_raster.dropna(), \"GDLCODE\", weights_col_name=\"population\", cols_to_agg=[\"centered_pred\",\"Sub-national HDI\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster, extent = rasterize_df(pre_raster, \n",
    "                              data_colname = \"clipped\", \n",
    "                              grid_delta=.1, \n",
    "                              lon_col=\"lon10\", \n",
    "                              lat_col=\"lat10\",\n",
    "                             custom_extent = (-180,180,-56,74)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checks on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff247b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raster, interpolation=\"nearest\", extent=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72028f",
   "metadata": {},
   "source": [
    "####  Write grid data product as a raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {'driver': 'GTiff',\n",
    " 'dtype': 'float64',\n",
    " 'nodata': np.nan,\n",
    " 'width': 3600,\n",
    " 'height': 1300,\n",
    " 'count': 1,\n",
    "'crs': \"EPSG:4326\",\n",
    "'transform': Affine(0.1, 0.0, extent[0],\n",
    "        0.0, -0.1, extent[3])\n",
    "       }\n",
    "\n",
    "raster_outpath = (data_dir + \"preds/\"\n",
    "           \"hdi_raster_predictions.tif\")\n",
    "\n",
    "with rasterio.open(raster_outpath , \"w\", **meta) as dest:\n",
    "     dest.write(np.array([raster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5cf12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
